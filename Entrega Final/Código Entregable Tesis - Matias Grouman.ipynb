{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "\n",
    "#Datasets#\n",
    "ds16= pd.read_csv('Downloads/raw/Estudiante_5-6 año Secundaria 2016.csv', sep= ',')\n",
    "dict_2016 = pd.read_excel('Downloads/raw/Diccionarios/Dic_2016_Sec.xlsx')\n",
    "\n",
    "#Bases Anexas#\n",
    "\n",
    "pob_escuelas = pd.read_csv('Downloads/gobiernoslocales_2020/poblacion.csv', sep=',')\n",
    "q_escuelas = pd.read_csv('Downloads/gobiernoslocales_2020/escuelas.csv', sep=',')\n",
    "dpto_arg = pd.read_csv('Downloads/gobiernoslocales_2020/departamentos.csv', sep=',')\n",
    "mun = pd.read_csv('Downloads/gobiernoslocales_2020/ign_municipio.csv', sep=';')\n",
    "dptos = pd.read_csv('Downloads/gobiernoslocales_2020/ign_departamento.csv', sep=';')\n",
    "dtos_match = pd.read_csv('Downloads/gobiernoslocales_2020/Dpto_transformado.csv', sep=';')\n",
    "dptos_merge = pd.read_csv('Downloads/Dptos BsAs Mod.csv', sep= ';')\n",
    "\n",
    "print(\"dataset total\",ds16.shape)\n",
    "print(\"dataset total\", ds16.size)\n",
    "print(\"diccionario\",dict_2016.shape)\n",
    "print(\"diccionario\", dict_2016.size)\n",
    "\n",
    "#Muestra BsAs y CABA#\n",
    "val_filt = [2, 6]\n",
    "df = ds16[ds16.cod_provincia.isin(val_filt)]\n",
    "df = df.infer_objects()\n",
    "\n",
    "print(\"muestra\", df.shape)\n",
    "print(\"muestra\", df.size)\n",
    "\n",
    "\n",
    "#Se agrega la información geográfica del Municipio#\n",
    "df2 = df.merge(dptos_merge, on =['Municipio'], how ='left',indicator = True) \n",
    "df2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametrización Gráfico#\n",
    "def plot_multiple_vars(df,x,y,title='Titulo de referencia',label= False):\n",
    "    \n",
    "    data =df[y].value_counts().to_frame().reset_index().sort_values(y)\n",
    "    \n",
    "    if list(label):\n",
    "        label = label[label.Variable==y].set_index('Códigos')\n",
    "        data['index'] =data['index'].map(label['Codigo_Et']) \n",
    "        \n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.barplot(data = data , \n",
    "                     x =x,y=y,order = data['index']\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "    ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "    \n",
    "    if list(label):\n",
    "        ax.set(xlabel= label['Variabl_Et'].unique().item(), ylabel='Count')\n",
    "    else:\n",
    "        ax.set(xlabel= y, ylabel='Count')\n",
    "    # add proper Dim values as x labels\n",
    "    for item in ax.get_xticklabels(): item.set_rotation(90)\n",
    "    for i, v in enumerate(data[y].iteritems()):        \n",
    "        ax.text(i ,v[1], \"{:,}\".format(v[1]), color='m', va ='bottom', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.title(title,size =20)\n",
    "    return plt.show()\n",
    "\n",
    "variables_explicitas =dict_2016[dict_2016.Variable.isin(ds16.columns)]\n",
    "variables_explicitas.head()\n",
    "len(variables_explicitas.Variable.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estadística Descriptiva y Correlaciones#\n",
    "desc = df2.describe(include = 'all').transpose()\n",
    "export_excel2= desc.to_excel('Downloads/Outputs/Summ_First_sinmuni.xlsx',index= True, header=True)\n",
    "corr = df2.corr(method = 'spearman')\n",
    "export_excel2= corr.to_excel('Downloads/Outputs/Corr_0310.xlsx',index= True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ratio = df2.isnull().sum().sum() / df2.size\n",
    "null_ratio = df2.isin([-9.0, -1.0,\"\"]).sum().sum() / df2.size\n",
    "print(\"missing ratio\", round(missing_ratio,2))\n",
    "print(\"null ratio\", round(null_ratio,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Variables por Categoría#\n",
    "plot = pd.crosstab(index=m['isocioa'],\n",
    "            columns=m['ldesemp']\n",
    "                  ).apply(lambda r: r/r.sum() *100,\n",
    "                          axis=0).plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Variables Pie Chart#\n",
    "plot = df2['ldesemp'].value_counts().plot(kind='pie', autopct='%.2f', \n",
    "                                            figsize=(6, 6),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfico por variable#\n",
    "plot_multiple_vars(df2, x ='index',y='Ap48a',title='¿Con qué frecuencia usas la computadora para trabajar en clase de Informática?',label = dict_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfico más de una variable#\n",
    "var = 'sector'\n",
    "ap = 'Subcategory'\n",
    "\n",
    "variables_explicitas.loc[variables_explicitas.Variable == 'cod_provincia']\n",
    "secundario_2016_sector_provincia = df[['cod_provincia',var]].copy()\n",
    "\n",
    "secundario_2016_sector_provincia['cod_provinca'] = secundario_2016_sector_provincia['cod_provincia']\\\n",
    "                                .map(variables_explicitas.loc[variables_explicitas.Variable == 'cod_provincia']\\\n",
    "                                .set_index('Códigos')['Codigo_Et'])\n",
    "\n",
    "secundario_2016_sector_provincia[var] = secundario_2016_sector_provincia[var]\\\n",
    "                                .map(variables_explicitas.loc[variables_explicitas.Variable == var]\\\n",
    "                                .set_index('Códigos')['Codigo_Et'])\n",
    "\n",
    "secundario_2016_sector_provincia['count'] = 1\n",
    "\n",
    "groupby_secundario_2016_sector_provincia = secundario_2016_sector_provincia\\\n",
    "                                            .groupby(['cod_provincia',var]).count().reset_index()\n",
    "    \n",
    "groupby_secundario_2016_sector_provincia.groupby(var).sum()['count']\n",
    "\n",
    "#\n",
    "data = (groupby_secundario_2016_sector_provincia.set_index([var,\n",
    "                                                     'cod_provincia'])['count'] / groupby_secundario_2016_sector_provincia.groupby('cod_provincia').sum()['count'])\\\n",
    ".to_frame().reset_index()\n",
    "\n",
    "\n",
    "#\n",
    "data= groupby_secundario_2016_sector_provincia.pivot(index=var,columns='cod_provincia',values='count')\n",
    "data.plot(kind='bar',figsize=(20,10), label=dict_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tablas#\n",
    "from tabulate import tabulate\n",
    "\n",
    "preg = 'sector'\n",
    "filt1 = ['CABA']\n",
    "filt2 = ['AMBA sin CABA']\n",
    "filt3 = ['Interior GBA']\n",
    "caba = df2[df2.Subcategory.isin(filt1)]\n",
    "gba = df2[df2.Subcategory.isin(filt2)]\n",
    "interior = df2[df2.Subcategory.isin(filt3)]\n",
    "\n",
    "privado_caba = caba.sector.value_counts()[2.0]\n",
    "privado_gba = gba.sector.value_counts()[2.0]\n",
    "privado_int = interior.sector.value_counts()[2.0]\n",
    "pub_caba = caba.sector.value_counts()[1.0]\n",
    "pub_gba = gba.sector.value_counts()[1.0]\n",
    "pub_int = interior.sector.value_counts()[1.0]\n",
    "m1 = privado_caba + pub_caba + privado_gba + pub_gba\n",
    "m3 = privado_int + pub_int\n",
    "mtot = privado_caba + pub_caba + privado_gba + pub_gba + privado_int + pub_int\n",
    "\n",
    "\n",
    "data = [(\"Privado AMBA\", (privado_caba + privado_gba) / m1),\n",
    "       (\"Privado Int GBA\", (privado_int) / m3),\n",
    "       (\"Publico AMBA\", (pub_caba+ pub_gba) / m1),\n",
    "       (\"Publico Int GBA\", (pub_int)/m3)] \n",
    "\n",
    "dataset= tabulate(data)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance#\n",
    "\n",
    "#Elimino Variables que no necesito#\n",
    "df3 = df2.copy()\n",
    "delete = pd.read_csv('Downloads/elimina.csv', sep= ';')\n",
    "col_list = delete['del']\n",
    "df3.drop(col_list,axis=True,inplace=True)\n",
    "df3.shape\n",
    "\n",
    "#Separo Target y Variables Predictoras#\n",
    "df3 = df3.replace('nan','NaN')\n",
    "df3 = df3.fillna(0)\n",
    "data = df3.dropna()\n",
    "features = [feat for feat in list(data) \n",
    "            if feat != 'ldesemp']\n",
    "\n",
    "datamat = np.array(features)\n",
    "X, y = data[features], data.ldesemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest#\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size= 0.1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "result = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42) \n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "tree_importance_sorted_idx = np.argsort(clf.feature_importances_) \n",
    "tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(50, 24))\n",
    "ax1.barh(tree_indices,\n",
    "         clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n",
    "ax1.set_yticklabels(datamat[tree_importance_sorted_idx])\n",
    "ax1.set_yticks(tree_indices)\n",
    "ax1.set_ylim((0, len(clf.feature_importances_)))\n",
    "ax2.boxplot(result.importances[perm_sorted_idx].T, vert=False,\n",
    "            labels=datamat[perm_sorted_idx])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST#\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# feature importance\n",
    "print(model.feature_importances_)\n",
    "\n",
    "plot_importance(model)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sort\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "# define custom class to fix bug in xgboost 1.0.2\n",
    "class MyXGBClassifier(XGBClassifier):\n",
    "               @property\n",
    "               def coef_(self):\n",
    "                              return None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=7)\n",
    "\n",
    "model = MyXGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Fit model using each importance as a threshold\n",
    "thresholds = sort(model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "               selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "               select_X_train = selection.transform(X_train)\n",
    "               # train model\n",
    "                selection_model = XGBClassifier()\n",
    "                selection_model.fit(select_X_train, y_train)\n",
    "                # eval model\n",
    "                select_X_test = selection.transform(X_test)\n",
    "                predictions = selection_model.predict(select_X_test)\n",
    "                accuracy = accuracy_score(y_test, predictions)\n",
    "                print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
